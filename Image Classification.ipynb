{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5RJ-ze9tOGMf",
    "outputId": "12965db0-7e9f-443f-dba6-50ca5b4f6b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/70\n",
      "50000/50000 [==============================] - 30s 599us/step - loss: 1.6950 - acc: 0.4217 - val_loss: 1.3074 - val_acc: 0.5473\n",
      "Epoch 2/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 1.2420 - acc: 0.5760 - val_loss: 1.0522 - val_acc: 0.6472\n",
      "Epoch 3/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 1.0451 - acc: 0.6506 - val_loss: 0.8555 - val_acc: 0.7215\n",
      "Epoch 4/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.9399 - acc: 0.6935 - val_loss: 0.9179 - val_acc: 0.7148\n",
      "Epoch 5/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.8789 - acc: 0.7189 - val_loss: 0.7902 - val_acc: 0.7490\n",
      "Epoch 6/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.8137 - acc: 0.7417 - val_loss: 0.7834 - val_acc: 0.7555\n",
      "Epoch 7/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.7656 - acc: 0.7616 - val_loss: 0.7156 - val_acc: 0.7884\n",
      "Epoch 8/70\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.7304 - acc: 0.7783 - val_loss: 0.8133 - val_acc: 0.7597\n",
      "Epoch 9/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.7022 - acc: 0.7883 - val_loss: 0.6971 - val_acc: 0.7952\n",
      "Epoch 10/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6716 - acc: 0.8053 - val_loss: 0.6405 - val_acc: 0.8166\n",
      "Epoch 11/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6452 - acc: 0.8159 - val_loss: 0.7197 - val_acc: 0.7994\n",
      "Epoch 12/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6332 - acc: 0.8246 - val_loss: 0.6864 - val_acc: 0.8086\n",
      "Epoch 13/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6112 - acc: 0.8338 - val_loss: 0.7246 - val_acc: 0.8103\n",
      "Epoch 14/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.5986 - acc: 0.8411 - val_loss: 0.7348 - val_acc: 0.8026\n",
      "Epoch 15/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.5859 - acc: 0.8469 - val_loss: 0.6384 - val_acc: 0.8382\n",
      "Epoch 16/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.5717 - acc: 0.8539 - val_loss: 0.6593 - val_acc: 0.8374\n",
      "Epoch 17/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.5611 - acc: 0.8618 - val_loss: 0.6474 - val_acc: 0.8361\n",
      "Epoch 18/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.5538 - acc: 0.8660 - val_loss: 0.6315 - val_acc: 0.8410\n",
      "Epoch 19/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.5472 - acc: 0.8684 - val_loss: 0.6412 - val_acc: 0.8489\n",
      "Epoch 20/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.5388 - acc: 0.8746 - val_loss: 0.6426 - val_acc: 0.8477\n",
      "Epoch 21/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.5321 - acc: 0.8777 - val_loss: 0.6330 - val_acc: 0.8498\n",
      "Epoch 22/70\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.5276 - acc: 0.8812 - val_loss: 0.6529 - val_acc: 0.8469\n",
      "Epoch 23/70\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.5200 - acc: 0.8832 - val_loss: 0.8109 - val_acc: 0.8197\n",
      "Epoch 24/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.5127 - acc: 0.8879 - val_loss: 0.6606 - val_acc: 0.8536\n",
      "Epoch 25/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.5062 - acc: 0.8899 - val_loss: 0.7194 - val_acc: 0.8455\n",
      "Epoch 26/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.5006 - acc: 0.8946 - val_loss: 0.6537 - val_acc: 0.8569\n",
      "Epoch 27/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.5008 - acc: 0.8946 - val_loss: 0.6892 - val_acc: 0.8510\n",
      "Epoch 28/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.4992 - acc: 0.8965 - val_loss: 0.6701 - val_acc: 0.8527\n",
      "Epoch 29/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.4921 - acc: 0.9002 - val_loss: 0.6968 - val_acc: 0.8488\n",
      "Epoch 30/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.4798 - acc: 0.9040 - val_loss: 0.7637 - val_acc: 0.8391\n",
      "Epoch 31/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4883 - acc: 0.9019 - val_loss: 0.6761 - val_acc: 0.8523\n",
      "Epoch 32/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4824 - acc: 0.9061 - val_loss: 0.6579 - val_acc: 0.8594\n",
      "Epoch 33/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4749 - acc: 0.9092 - val_loss: 0.7014 - val_acc: 0.8567\n",
      "Epoch 34/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.4726 - acc: 0.9097 - val_loss: 0.7068 - val_acc: 0.8497\n",
      "Epoch 35/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.4712 - acc: 0.9119 - val_loss: 0.7038 - val_acc: 0.8487\n",
      "Epoch 36/70\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.4709 - acc: 0.9117 - val_loss: 0.7198 - val_acc: 0.8474\n",
      "Epoch 37/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.4677 - acc: 0.9129 - val_loss: 0.6650 - val_acc: 0.8625\n",
      "Epoch 38/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.4654 - acc: 0.9138 - val_loss: 0.6424 - val_acc: 0.8672\n",
      "Epoch 39/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.4558 - acc: 0.9167 - val_loss: 0.6973 - val_acc: 0.8569\n",
      "Epoch 40/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.4567 - acc: 0.9179 - val_loss: 0.6668 - val_acc: 0.8654\n",
      "Epoch 41/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4574 - acc: 0.9178 - val_loss: 0.6916 - val_acc: 0.8633\n",
      "Epoch 42/70\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.4507 - acc: 0.9189 - val_loss: 0.7421 - val_acc: 0.8547\n",
      "Epoch 43/70\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.4505 - acc: 0.9202 - val_loss: 0.6644 - val_acc: 0.8617\n",
      "Epoch 44/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4456 - acc: 0.9213 - val_loss: 0.6825 - val_acc: 0.8640\n",
      "Epoch 45/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4416 - acc: 0.9222 - val_loss: 0.7321 - val_acc: 0.8483\n",
      "Epoch 46/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4394 - acc: 0.9231 - val_loss: 0.6748 - val_acc: 0.8654\n",
      "Epoch 47/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.4371 - acc: 0.9254 - val_loss: 0.6920 - val_acc: 0.8629\n",
      "Epoch 48/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4390 - acc: 0.9243 - val_loss: 0.6585 - val_acc: 0.8691\n",
      "Epoch 49/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4381 - acc: 0.9256 - val_loss: 0.6990 - val_acc: 0.8612\n",
      "Epoch 50/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4351 - acc: 0.9248 - val_loss: 0.7571 - val_acc: 0.8536\n",
      "Epoch 51/70\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.4345 - acc: 0.9267 - val_loss: 0.7045 - val_acc: 0.8663\n",
      "Epoch 52/70\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.4323 - acc: 0.9263 - val_loss: 0.7307 - val_acc: 0.8515\n",
      "Epoch 53/70\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.4315 - acc: 0.9281 - val_loss: 0.6662 - val_acc: 0.8665\n",
      "Epoch 54/70\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.4279 - acc: 0.9293 - val_loss: 0.7180 - val_acc: 0.8547\n",
      "Epoch 55/70\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.4241 - acc: 0.9302 - val_loss: 0.7222 - val_acc: 0.8607\n",
      "Epoch 56/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4240 - acc: 0.9301 - val_loss: 0.6947 - val_acc: 0.8641\n",
      "Epoch 57/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.4219 - acc: 0.9286 - val_loss: 0.7287 - val_acc: 0.8541\n",
      "Epoch 58/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4242 - acc: 0.9294 - val_loss: 0.6432 - val_acc: 0.8696\n",
      "Epoch 59/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.4172 - acc: 0.9324 - val_loss: 0.6791 - val_acc: 0.8615\n",
      "Epoch 60/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.4186 - acc: 0.9324 - val_loss: 0.6400 - val_acc: 0.8717\n",
      "Epoch 61/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.4159 - acc: 0.9330 - val_loss: 0.6534 - val_acc: 0.8701\n",
      "Epoch 62/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4132 - acc: 0.9325 - val_loss: 0.6807 - val_acc: 0.8670\n",
      "Epoch 63/70\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.4150 - acc: 0.9331 - val_loss: 0.6995 - val_acc: 0.8671\n",
      "Epoch 64/70\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.4122 - acc: 0.9340 - val_loss: 0.6711 - val_acc: 0.8702\n",
      "Epoch 65/70\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.4092 - acc: 0.9340 - val_loss: 0.6617 - val_acc: 0.8698\n",
      "Epoch 66/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.4048 - acc: 0.9366 - val_loss: 0.6468 - val_acc: 0.8771\n",
      "Epoch 67/70\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.4070 - acc: 0.9344 - val_loss: 0.6726 - val_acc: 0.8702\n",
      "Epoch 68/70\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.4030 - acc: 0.9361 - val_loss: 0.6860 - val_acc: 0.8711\n",
      "Epoch 69/70\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.4076 - acc: 0.9349 - val_loss: 0.6412 - val_acc: 0.8736\n",
      "Epoch 70/70\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.4063 - acc: 0.9361 - val_loss: 0.6631 - val_acc: 0.8738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f584f778128>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 70\n",
    "#data_augmentation = True\n",
    "num_predictions = 20\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay),input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3),padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3),padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3),padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ioxGCbNWFJj0",
    "outputId": "01609e41-70b6-4cfd-e364-e5e7e31b278f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 281us/step\n",
      "Test loss: 0.663082053899765\n",
      "Test accuracy: 0.8738\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Team Assignment 2 Team9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
